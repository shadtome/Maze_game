{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training area for agents in Hunger Maze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/cody/Documents/DataSciBC/Generative_AI/maze_game/TrainingGround\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "import matplotlib.pyplot as plt\n",
    "import maze_generator.maze_dataset as md\n",
    "from Maze_env.reward_functions.maze_runner import MazeRunnerRewardsFun\n",
    "import numpy as np\n",
    "from DQN.training.basic import BaseTraining\n",
    "from DQN.agents.hungermaze import HungerGamesAgent\n",
    "from DQN.agents.basic import BaseAgent\n",
    "from Maze_env.game_info import basic_info\n",
    "from Maze_env.reward_functions.hunger_games import HungerGamesRewardsFun\n",
    "from Maze_env.wrappers.reward_wrappers.hunger_games_rewards import HungerGamesRewards\n",
    "\n",
    "from Maze_env.game_info.hunger_games_info import HungerGames\n",
    "from Maze_env.game_info.team_hunger_games_info import TeamHungerGames\n",
    "\n",
    "import DQN.models.base as base\n",
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Mazes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = md.Maze_dataset(1,(3,3),maze_type = 'percolation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# example of the maze\n",
    "dataset.show_maze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for the agents\n",
    "n_agents = 1\n",
    "vision = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward Distribution for object agents\n",
      "----------------Rewards Distribution --------------\n",
      "HIT_OTHER: -500.0\n",
      "TOO_CLOSE: -0.01\n",
      "TOO_CLOSE_CONSTANT: -0.01\n",
      "SEE_GOAL: 0.0\n",
      "DONT_SEE_GOAL: -0.0\n",
      "NEW_PLACE: 50.0\n",
      "OLD_PLACE: -0.75\n",
      "GET_CLOSER: 50.0\n",
      "GET_CLOSER_CONSTANT: 50.0\n",
      "GET_FARTHER: -30.0\n",
      "GET_FARTHER_CONSTANT: -30.0\n",
      "DIST: 0.0\n",
      "GOAL: 100.0\n",
      "FAIL: -1.0\n",
      "-----------------------------------------------------\n",
      "----------------------------------\n",
      "Basic epsilon decay scheduler:\n",
      "Start epsilon: 1\n",
      "End epsilon: 0.05\n",
      "Decay total: 300000\n",
      "Decay rate: 0.0009935761740440755\n",
      "\n",
      "------------------------------\n",
      "Group 0: Learning rate = 0.0005\n",
      "--------------------------------------\n",
      "Basis learning rate scheduler:\n",
      "Step size: 5000\n",
      "Gamma: 0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cody/miniconda3/envs/GameRFL/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:245: UserWarning: \u001b[33mWARN: The reward returned by `step()` must be a float, int, np.integer or np.floating, actual type: <class 'dict'>\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 59\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# --- trainer of the agent --- #\u001b[39;00m\n\u001b[1;32m     33\u001b[0m train \u001b[38;5;241m=\u001b[39m BaseTraining(name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFFAHG_training\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     34\u001b[0m                               maze_dataset \u001b[38;5;241m=\u001b[39m dataset,\n\u001b[1;32m     35\u001b[0m                               maze_agent \u001b[38;5;241m=\u001b[39m maze_agent,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m                               frame_mult\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.5\u001b[39m,\n\u001b[1;32m     58\u001b[0m                               )\n\u001b[0;32m---> 59\u001b[0m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mpeak\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/DataSciBC/Generative_AI/maze_game/DQN/training/basic.py:551\u001b[0m, in \u001b[0;36mBaseTraining.train\u001b[0;34m(self, test_agent, peak)\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_on[obj_type]:\n\u001b[1;32m    548\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_objects[obj_type]):\n\u001b[1;32m    549\u001b[0m \n\u001b[1;32m    550\u001b[0m         \u001b[38;5;66;03m# --- add experience to replay buffer --- #\u001b[39;00m\n\u001b[0;32m--> 551\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend_to_RB\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[43mobj_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlocal_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43ma\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[43mobj_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mglobal_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43ma\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43mobj_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43ma\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m[\u001b[49m\u001b[43mobj_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlocal_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43ma\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m[\u001b[49m\u001b[43mobj_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mglobal_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43ma\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m[\u001b[49m\u001b[43mobj_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43ma\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mterminated\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnext_info\u001b[49m\u001b[43m[\u001b[49m\u001b[43mobj_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43ma\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43mobj_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    555\u001b[0m         \u001b[38;5;66;03m# --- accumulate rewards --- #\u001b[39;00m\n\u001b[1;32m    556\u001b[0m         cum_reward[obj_type][a] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward[obj_type][a]\n",
      "File \u001b[0;32m~/Documents/DataSciBC/Generative_AI/maze_game/DQN/training/basic.py:376\u001b[0m, in \u001b[0;36mBaseTraining.append_to_RB\u001b[0;34m(self, local_s, global_s, action, n_local_s, n_global_s, reward, terminated, info, agent_id, obj_type)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mappend_to_RB\u001b[39m(\u001b[38;5;28mself\u001b[39m,local_s,global_s,action,n_local_s,n_global_s,reward,terminated,info,agent_id,obj_type):\n\u001b[1;32m    374\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Append the experience to the replay buffer depending on the type of buffer\"\"\"\u001b[39;00m\n\u001b[0;32m--> 376\u001b[0m     td_e \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtd_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_s\u001b[49m\u001b[43m,\u001b[49m\u001b[43mglobal_s\u001b[49m\u001b[43m,\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43mn_local_s\u001b[49m\u001b[43m,\u001b[49m\u001b[43mn_global_s\u001b[49m\u001b[43m,\u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43mterminated\u001b[49m\u001b[43m,\u001b[49m\u001b[43minfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43mobj_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtd_errors[obj_type][agent_id]\u001b[38;5;241m.\u001b[39mappend(td_e)\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mper:\n",
      "File \u001b[0;32m~/Documents/DataSciBC/Generative_AI/maze_game/DQN/training/basic.py:353\u001b[0m, in \u001b[0;36mBaseTraining.td_error\u001b[0;34m(self, local_s, global_s, action, n_local_s, n_global_s, reward, terminated, info, obj_type)\u001b[0m\n\u001b[1;32m    350\u001b[0m local_s,global_s,action,n_local_s,n_global_s,reward,terminated,info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(local_s,global_s,action,n_local_s,n_global_s,reward,terminated,info,single\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,obj_type\u001b[38;5;241m=\u001b[39mobj_type)\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# --- get Q(s,a) for each action --- #\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapplyPolicyQ_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_s\u001b[49m\u001b[43m,\u001b[49m\u001b[43mglobal_s\u001b[49m\u001b[43m,\u001b[49m\u001b[43minfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43mobj_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;66;03m# --- pick the Q values corresponding to the picked actions --- #\u001b[39;00m\n\u001b[1;32m    357\u001b[0m selected_q_values \u001b[38;5;241m=\u001b[39m q_values\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m1\u001b[39m,action\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/DataSciBC/Generative_AI/maze_game/DQN/training/basic.py:342\u001b[0m, in \u001b[0;36mBaseTraining.applyPolicyQ_fun\u001b[0;34m(self, local_s, global_s, info, obj_type)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapplyPolicyQ_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m,local_s,global_s,info,obj_type):\n\u001b[0;32m--> 342\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magents\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mQ_fun\u001b[49m\u001b[43m[\u001b[49m\u001b[43mobj_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_s\u001b[49m\u001b[43m,\u001b[49m\u001b[43mglobal_s\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/GameRFL/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/GameRFL/lib/python3.12/site-packages/torch/nn/modules/module.py:1741\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1740\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 1741\u001b[0m     forward_call \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_tracing_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward)\n\u001b[1;32m   1742\u001b[0m     \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m             \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m             \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "\n",
    "# --- Agents for the Hunger Games --- #\n",
    "rewards = rewards = HungerGamesRewardsFun(\n",
    "                        GOAL = 100.0,\n",
    "                         SEE_GOAL = 0.00,\n",
    "                         DONT_SEE_GOAL = -0.00,\n",
    "                         NEW_PLACE = 50.0,\n",
    "                         OLD_PLACE = -0.75,\n",
    "                         GET_CLOSER = 50.0, \n",
    "                         GET_CLOSER_CONSTANT = 50.0,\n",
    "                         GET_FARTHER = -30.0,\n",
    "                         GET_FARTHER_CONSTANT = -30.0,\n",
    "                         DIST = 0.0,\n",
    "                         HIT_OTHER = -500.0,\n",
    "                         TOO_CLOSE = -0.01,\n",
    "                         TOO_CLOSE_CONSTANT = -0.01,\n",
    "                )\n",
    "game_info = HungerGames()\n",
    "game_info.update_game_info(\n",
    "                           rewards_dist = {'agents': rewards}\n",
    "\n",
    ")\n",
    "maze_agent =BaseAgent({'agents':base.CNN_version1},\n",
    "                               vision={'agents':vision},\n",
    "                               action_type='cardinal',\n",
    "                               dist_paradigm='path',\n",
    "                               game_info=game_info\n",
    ")\n",
    "\n",
    "# --- trainer of the agent --- #\n",
    "train = BaseTraining(name = 'FFAHG_training',\n",
    "                              maze_dataset = dataset,\n",
    "                              maze_agent = maze_agent,\n",
    "                              len_game=50,\n",
    "                              n_objects={'agents':n_agents},\n",
    "                              final_epsilon = 0.05,\n",
    "                              gamma = 0.99,\n",
    "                              tau = 0.0001,\n",
    "                               batch_size = 64,\n",
    "                              n_frames = 500000,\n",
    "                              lr = 0.0005,\n",
    "                              lr_step_size=5000,\n",
    "                              lr_gamma = 0.98,\n",
    "                              lr_head_gamma = 0.98,\n",
    "                              l2_regular=0.01,\n",
    "                              replay_buffer_size=300000,\n",
    "                              replay_buffer_min_perc=0.1,\n",
    "                              target_update=5000,\n",
    "                              policy_update=1,\n",
    "                              lambda_entropy=0.1,\n",
    "                              beta = 0.4,\n",
    "                              alpha = 0.6,\n",
    "                              decay_total = 300000,\n",
    "                              per = True,\n",
    "                              frame_mult=1.5,\n",
    "                              )\n",
    "train.train(test_agent=False,peak=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'bogus'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m train\u001b[38;5;241m.\u001b[39mresults()\n",
      "File \u001b[0;32m~/Documents/DataSciBC/Generative_AI/maze_game/DQN/training/basic.py:786\u001b[0m, in \u001b[0;36mBaseTraining.save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(object_fd_best)\u001b[38;5;241m==\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    785\u001b[0m         os\u001b[38;5;241m.\u001b[39mmkdir(object_fd_best)\n\u001b[0;32m--> 786\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m[\u001b[49m\u001b[43mobj_type\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39msave(object_fd_original)\n\u001b[1;32m    787\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler[obj_type]\u001b[38;5;241m.\u001b[39msave(object_fd_best)\n\u001b[1;32m    789\u001b[0m \u001b[38;5;66;03m# now to save the hyperparameters for this mod\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'bogus'"
     ]
    }
   ],
   "source": [
    "train.save()\n",
    "train.results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cody/miniconda3/envs/GameRFL/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:245: UserWarning: \u001b[33mWARN: The reward returned by `step()` must be a float, int, np.integer or np.floating, actual type: <class 'dict'>\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'agents': 0.12500000000000008}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.agents.test_agent(dataset,n_episodes=1000,len_game=15,num_objects={'agents':n_agents})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cumulative reward: {'agents': [-106.94444444444444, -101.38888888888889]}\n",
      "cumulative reward: {'agents': [-25.25, -26.572222222222223]}\n",
      "cumulative reward: {'agents': [-110.07757201646089, -106.58251028806585]}\n",
      "cumulative reward: {'agents': [-117.77592592592593, -98.88703703703702]}\n",
      "cumulative reward: {'agents': [-106.94444444444444, -98.88703703703702]}\n",
      "cumulative reward: {'agents': [-106.94444444444444, -106.58251028806585]}\n",
      "cumulative reward: {'agents': [-112.18950617283949, -101.38888888888889]}\n",
      "cumulative reward: {'agents': [-110.07757201646089, -109.16193415637858]}\n",
      "cumulative reward: {'agents': [-33.44444444444444, -27.9417695473251]}\n",
      "cumulative reward: {'agents': [-106.58251028806585, -117.10432098765432]}\n",
      "cumulative reward: {'agents': [-74.91296296296298, -98.88703703703702]}\n",
      "cumulative reward: {'agents': [-112.49999999999999, -112.49999999999999]}\n",
      "cumulative reward: {'agents': [-103.71954732510288, -101.38888888888889]}\n",
      "cumulative reward: {'agents': [-30.328600823045267, -32.33333333333333]}\n",
      "cumulative reward: {'agents': [-102.330658436214, -103.71954732510288]}\n",
      "cumulative reward: {'agents': [-106.94444444444444, -106.94444444444444]}\n",
      "cumulative reward: {'agents': [-8.78641975308642, -8.78641975308642]}\n",
      "cumulative reward: {'agents': [-98.88703703703702, -112.18950617283949]}\n",
      "cumulative reward: {'agents': [-25.00843621399177, -25.99074074074074]}\n",
      "cumulative reward: {'agents': [-102.330658436214, -74.91296296296298]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train.agents.run_agent(dataset[0],num_objects={'agents':n_agents},n_episodes=20,len_game=15,epsilon=0,init_pos={})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GameRFL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
